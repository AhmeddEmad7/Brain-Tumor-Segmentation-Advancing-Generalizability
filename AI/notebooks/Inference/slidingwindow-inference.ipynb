{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T20:59:45.548332Z",
     "iopub.status.busy": "2025-02-09T20:59:45.548092Z",
     "iopub.status.idle": "2025-02-09T21:00:04.645772Z",
     "shell.execute_reply": "2025-02-09T21:00:04.644730Z",
     "shell.execute_reply.started": "2025-02-09T20:59:45.548305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-09T21:00:04.649245Z",
     "iopub.status.busy": "2025-02-09T21:00:04.648763Z",
     "iopub.status.idle": "2025-02-09T21:00:43.868123Z",
     "shell.execute_reply": "2025-02-09T21:00:43.867396Z",
     "shell.execute_reply.started": "2025-02-09T21:00:04.649202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "\n",
    "\n",
    "class UnetBasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN module module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        stride: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            dropout=dropout,\n",
    "            conv_only=True,\n",
    "        )\n",
    "\n",
    "        self.conv2 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            conv_only=True\n",
    "        )\n",
    "        self.lrelu = get_act_layer(name=act_name)\n",
    "        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        out = self.norm1(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetUpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "        trans_bias: transposed convolution bias.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        upsample_kernel_size: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "        trans_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        upsample_stride = upsample_kernel_size\n",
    "        \n",
    "        # ( a purple arrow in the paper )\n",
    "        self.transp_conv = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=upsample_kernel_size,\n",
    "            stride=upsample_stride,\n",
    "            dropout=dropout,\n",
    "            bias=trans_bias,\n",
    "            conv_only=True,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "        \n",
    "        # A light blue conv blocks in the decoder of nnUNet\n",
    "        self.conv_block = UnetBasicBlock(\n",
    "            spatial_dims,\n",
    "            out_channels + out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            norm_name=norm_name,\n",
    "            act_name=act_name,\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, skip):\n",
    "        # number of channels for skip should equals to out_channels\n",
    "        out = self.transp_conv(inp)\n",
    "        out = torch.cat((out, skip), dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetOutBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, in_channels: int, out_channels: int, dropout: Optional[Union[Tuple, str, float]] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = get_conv_layer(\n",
    "            spatial_dims, in_channels, out_channels, kernel_size=1, stride=1, dropout=dropout, bias=True, conv_only=True\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.conv(inp)\n",
    "    \n",
    "\n",
    "def get_conv_layer(\n",
    "    spatial_dims: int,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: Union[Sequence[int], int] = 3,\n",
    "    stride: Union[Sequence[int], int] = 1,\n",
    "    act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
    "    norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "    dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    bias: bool = False,\n",
    "    conv_only: bool = True,\n",
    "    is_transposed: bool = False,\n",
    "):\n",
    "    padding = get_padding(kernel_size, stride)\n",
    "    output_padding = None\n",
    "    if is_transposed:\n",
    "        output_padding = get_output_padding(kernel_size, stride, padding)\n",
    "    \n",
    "    return Convolution(\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        strides=stride,\n",
    "        kernel_size=kernel_size,\n",
    "        act=act,\n",
    "        norm=norm,\n",
    "        dropout=dropout,\n",
    "        bias=bias,\n",
    "        conv_only=conv_only,\n",
    "        is_transposed=is_transposed,\n",
    "        padding=padding,\n",
    "        output_padding=output_padding,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = (kernel_size_np - stride_np + 1) / 2\n",
    "    if np.min(padding_np) < 0:\n",
    "        raise AssertionError(\"padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    padding = tuple(int(p) for p in padding_np)\n",
    "\n",
    "    return padding if len(padding) > 1 else padding[0]\n",
    "\n",
    "\n",
    "def get_output_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = np.atleast_1d(padding)\n",
    "\n",
    "    out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
    "    if np.min(out_padding_np) < 0:\n",
    "        raise AssertionError(\"out_padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    out_padding = tuple(int(p) for p in out_padding_np)\n",
    "\n",
    "    return out_padding if len(out_padding) > 1 else out_padding[0]\n",
    "\n",
    "\n",
    "class DynUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        deep_supervision: bool,\n",
    "        KD: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.KD_enabled = KD\n",
    "        \n",
    "        self.input_conv = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=self.in_channels,\n",
    "                                     out_channels=64,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1\n",
    "                                     )\n",
    "        self.down1 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=64,\n",
    "                                     out_channels=96,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2 # Reduces spatial dims by 2\n",
    "                                     )\n",
    "        self.down2 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=96,\n",
    "                                     out_channels=128,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down3 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=128,\n",
    "                                     out_channels=192,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down4 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=192,\n",
    "                                     out_channels=256,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down5 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=256,\n",
    "                                     out_channels=384,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.bottleneck = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=384,\n",
    "                                     out_channels=512,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.up1 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=512,\n",
    "                                out_channels=384,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up2 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=384,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up3 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=256,\n",
    "                                out_channels=192,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up4 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=192,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        \n",
    "        self.up5 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=128,\n",
    "                                out_channels=96,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )        \n",
    "        self.up6 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=96,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.out1 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=64,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out2 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=96,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out3 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=128,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        \n",
    "    def forward( self, input ):\n",
    "        \n",
    "        # Input\n",
    "        x0 = self.input_conv( input ) # x0.shape = (B x 64 x 128 x 128 x 128) or (B x 64 x 192 x 192 x 128)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.down1( x0 ) # x1.shape = (B x 96 x 64 x 64 x 64)  or (B x 96 x 96 x 96 x 64)\n",
    "        x2 = self.down2( x1 ) # x2.shape = (B x 128 x 32 x 32 x 32) or (B x 128 x 48 x 48 x 32)\n",
    "        x3 = self.down3( x2 ) # x3.shape = (B x 192 x 16 x 16 x 16) or (B x 192 x 24 x 24 x 16)\n",
    "        x4 = self.down4( x3 ) # x4.shape = (B x 256 x 8 x 8 x 8)    or (B x 256 x 12 x 12 x 8)\n",
    "        x5 = self.down5( x4 ) # x5.shape = (B x 384 x 4 x 4 x 4)    or (B x 384 x 6 x 6 x 4)\n",
    "        \n",
    "        # Bottle-neck\n",
    "        x6 = self.bottleneck( x5 ) # x6.shape = (B x 512 x 2 x 2 x 2) or (B x 512 x 3 x 3 x 2)\n",
    "        \n",
    "        # Decoder\n",
    "        x7  = self.up1( x6, x5 )  # x7.shape  = (B x 384 x 4 x 4 x 4) or (B x 64 x 192 x 192 x 128)\n",
    "        x8  = self.up2( x7, x4 )  # x8.shape  = (B x 256 x 8 x 8 x 8) or (B x 64 x 192 x 192 x 128)\n",
    "        x9  = self.up3( x8, x3 )  # x9.shape  = (B x 192 x 16 x 16 x 16) or (B x 64 x 192 x 192 x 128)\n",
    "        x10 = self.up4( x9, x2 )  # x10.shape = (B x 128 x 32 x 32 x 32) or (B x 64 x 192 x 192 x 128)\n",
    "        x11 = self.up5( x10, x1 ) # x11.shape = (B x 96 x 64 x 64 x 64) or (B x 64 x 192 x 192 x 128)\n",
    "        x12 = self.up6( x11, x0 ) # x12.shape = (B x 64 x 128 x 128 x 128) or (B x 64 x 192 x 192 x 128)\n",
    "        \n",
    "        # Output\n",
    "        output = self.out1( x12 )\n",
    "        return { 'pred' : output }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:00:43.869782Z",
     "iopub.status.busy": "2025-02-09T21:00:43.869158Z",
     "iopub.status.idle": "2025-02-09T21:00:43.877553Z",
     "shell.execute_reply": "2025-02-09T21:00:43.876749Z",
     "shell.execute_reply.started": "2025-02-09T21:00:43.869754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from monai.transforms import LoadImage, Compose, NormalizeIntensityd, RandSpatialCropd, RandFlipd, \\\n",
    "                             RandRotate90d, Rand3DElasticd, RandAdjustContrastd, CenterSpatialCropd,\\\n",
    "                             Resized, RandRotated, Spacingd, CropForegroundd, SpatialPadd, AsDiscrete\n",
    "\n",
    "def load_sequences_from_paths(t1c_path, t1n_path, t2f_path, t2w_path):   # lazem yekon el path .nii aw .nii.gz     \n",
    "    loadimage = LoadImage(reader='NibabelReader', image_only=False)\n",
    "    \n",
    "    t1c_loader, t1c_metadata = loadimage( t1c_path )\n",
    "    t1n_loader, t1n_metadata = loadimage( t1n_path )\n",
    "    t2f_loader, t2f_metadata = loadimage( t2f_path )\n",
    "    t2w_loader, t2w_metadata = loadimage( t2w_path )\n",
    "\n",
    "    metadata = [t1c_metadata, t1n_metadata, t2f_metadata, t2w_metadata]\n",
    "\n",
    "    t1c_tensor = torch.Tensor(t1c_loader).unsqueeze(0)\n",
    "    t1n_tensor = torch.Tensor(t1n_loader).unsqueeze(0)\n",
    "    t2f_tensor = torch.Tensor(t2f_loader).unsqueeze(0)\n",
    "    t2w_tensor = torch.Tensor(t2w_loader).unsqueeze(0)\n",
    "\n",
    "    concat_tensor = torch.cat( (t1c_tensor, t1n_tensor, t2f_tensor, t2w_tensor), 0 )\n",
    "    raw_data = {'imgs' : np.array(concat_tensor[:,:,:,:])}\n",
    "    int_volumes = {'imgs' : torch.from_numpy(raw_data['imgs']).type(torch.IntTensor)}\n",
    "\n",
    "    processed_data = preprocess_data(raw_data)\n",
    "    norm_imgs  = np.array(processed_data['imgs'])\n",
    "    float_volumes = {'imgs' : torch.from_numpy(norm_imgs).type(torch.FloatTensor)}\n",
    "    \n",
    "    return float_volumes, int_volumes, metadata\n",
    "\n",
    "def preprocess_data(data):\n",
    "    transform = Compose([\n",
    "            # CropForegroundd(keys=[\"imgs\"], source_key=\"imgs\"),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True)\n",
    "        ])\n",
    "\n",
    "    preprocessed_data = transform(data)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:19:24.775367Z",
     "iopub.status.busy": "2025-02-09T21:19:24.774492Z",
     "iopub.status.idle": "2025-02-09T21:19:24.787837Z",
     "shell.execute_reply": "2025-02-09T21:19:24.786917Z",
     "shell.execute_reply.started": "2025-02-09T21:19:24.775336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from monai.transforms import AsDiscrete\n",
    "from pathlib import Path\n",
    "import os\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "def load_model(model_path):\n",
    "    model_path = Path(model_path)\n",
    "    model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False)       \n",
    "    if (model_path).is_file():\n",
    "        print(f\"Found model: {model_path}\")\n",
    "        ckpt = torch.load(model_path, map_location='cuda', weights_only=True)\n",
    "        model.load_state_dict(ckpt['student_model'])\n",
    "        print(f\"Loaded model: {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_prediction_mask(pred):\n",
    "    output_probs = (torch.sigmoid(pred) > 0.5)\n",
    "    _, _, H, W, D = output_probs.shape\n",
    "\n",
    "    output = output_probs[0]\n",
    "    seg_mask = torch.zeros((H, W, D))\n",
    "\n",
    "    seg_mask[torch.where(output[1, ...] == 1)] = 2  # WT --> ED\n",
    "    seg_mask[torch.where(output[2, ...] == 1)] = 1  # TC --> NCR\n",
    "    seg_mask[torch.where(output[3, ...] == 1)] = 3  # ET --> ET\n",
    "\n",
    "    return seg_mask.float()\n",
    "    \n",
    "def save_nifti_volumes(int_volumes, metadata, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sequence_names = ['t1c', 't1n', 't2f', 't2w']\n",
    "    \n",
    "    for i in range(len(metadata)):\n",
    "        nifti_image = nib.Nifti1Image(int_volumes['imgs'][i].numpy(), affine=metadata[i]['affine'])\n",
    "        file_name = f\"{sequence_names[i]}.nii.gz\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        nib.save(nifti_image, file_path)\n",
    "        print(f\"Saved: {file_path}\")\n",
    "\n",
    "def inference(t1c_path, t1n_path, t2f_path, t2w_path, output_dir, model_path):\n",
    "    input_data, int_volumes, metadata = load_sequences_from_paths(t1c_path, t1n_path, t2f_path, t2w_path)\n",
    "    save_nifti_volumes(int_volumes, metadata, output_dir)\n",
    "    \n",
    "    input_data['imgs'] = input_data['imgs'].unsqueeze(0).to('cuda')\n",
    "    print(\"Input to model shape:\", input_data['imgs'].shape)\n",
    "\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = sliding_window_inference(\n",
    "            inputs=input_data['imgs'],\n",
    "            roi_size=(128, 128, 128),\n",
    "            sw_batch_size=4,\n",
    "            predictor=model,\n",
    "            overlap=0.25,\n",
    "            mode='gaussian'\n",
    "        )\n",
    "    \n",
    "    prediction = generate_prediction_mask(output['pred'])\n",
    "    print(\"Prediction shape:\", prediction.shape)\n",
    "\n",
    "    # Saving prediction\n",
    "    nifti_pred = nib.Nifti1Image(prediction.cpu().numpy(), affine=metadata[0]['affine'])\n",
    "    nifti_pred.header.set_intent('label', name='Label Map')\n",
    "    \n",
    "    # Save the NIfTI file and _label left as it is\n",
    "    nib.save(nifti_pred, os.path.join(output_dir, f\"prediction_label.nii.gz\"))\n",
    "\n",
    "    return np.array(prediction.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T21:19:30.778305Z",
     "iopub.status.busy": "2025-02-09T21:19:30.777673Z",
     "iopub.status.idle": "2025-02-09T21:19:41.201188Z",
     "shell.execute_reply": "2025-02-09T21:19:41.200308Z",
     "shell.execute_reply.started": "2025-02-09T21:19:30.778272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/output/t1c.nii.gz\n",
      "Saved: /kaggle/working/output/t1n.nii.gz\n",
      "Saved: /kaggle/working/output/t2f.nii.gz\n",
      "Saved: /kaggle/working/output/t2w.nii.gz\n",
      "Input to model shape: torch.Size([1, 4, 240, 240, 155])\n",
      "Found model: /kaggle/input/gliomateachernewlabels/Teacher_model_after_epoch_100_trainLoss_0.5972_valLoss_0.3019.pth\n",
      "Loaded model: /kaggle/input/gliomateachernewlabels/Teacher_model_after_epoch_100_trainLoss_0.5972_valLoss_0.3019.pth\n",
      "Prediction shape: torch.Size([240, 240, 155])\n",
      "Inference done!\n"
     ]
    }
   ],
   "source": [
    "t1c_path = '/kaggle/input/bratsglioma/Training/BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1c.nii/00000057_brain_t1ce.nii'\n",
    "t1n_path = '/kaggle/input/bratsglioma/Training/BraTS-GLI-00000-000/BraTS-GLI-00000-000-t1n.nii/00000057_brain_t1.nii'\n",
    "t2f_path = '/kaggle/input/bratsglioma/Training/BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2f.nii/00000057_brain_flair.nii'\n",
    "t2w_path = '/kaggle/input/bratsglioma/Training/BraTS-GLI-00000-000/BraTS-GLI-00000-000-t2w.nii/00000057_brain_t2.nii'\n",
    "# seg_path = '/kaggle/input/bratsglioma/Training/BraTS-GLI-00006-000/BraTS-GLI-00006-000-seg.nii'\n",
    "\n",
    "output_dir = '/kaggle/working/output'\n",
    "model_path = '/kaggle/working/model.pth'\n",
    "prediction = inference(t1c_path, t1n_path, t2f_path, t2w_path, output_dir, model_path)\n",
    "print('Inference done!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5929819,
     "sourceId": 9697878,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6235428,
     "sourceId": 10107956,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
