{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Loading Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:27:37.772227Z",
     "iopub.status.busy": "2024-11-29T22:27:37.77159Z",
     "iopub.status.idle": "2024-11-29T22:28:29.19711Z",
     "shell.execute_reply": "2024-11-29T22:28:29.196366Z",
     "shell.execute_reply.started": "2024-11-29T22:27:37.772185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install monai\n",
    "\n",
    "import nibabel as nib\n",
    "from monai.transforms import LoadImage, Compose, NormalizeIntensityd, RandSpatialCropd, RandFlipd, \\\n",
    "                             RandRotate90d, Rand3DElasticd, RandAdjustContrastd, CenterSpatialCropd,\\\n",
    "                             Resized, RandRotated, RandZoomd, RandGaussianNoised, Spacingd, RandShiftIntensityd,  CropForegroundd, SpatialPadd, AsDiscrete, GridPatchd\\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate\n",
    "import pdb\n",
    "\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "from monai.networks import one_hot\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from grpc import insecure_channel\n",
    "import argparse\n",
    "from torch import optim, amp\n",
    "from monai.losses import DiceLoss\n",
    "import torch.distributed as dist\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pdb\n",
    "import logging\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.199862Z",
     "iopub.status.busy": "2024-11-29T22:28:29.199074Z",
     "iopub.status.idle": "2024-11-29T22:28:29.217863Z",
     "shell.execute_reply": "2024-11-29T22:28:29.217017Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.199815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset3D(Dataset):\n",
    "    def __init__(self, data_dir, patient_lists, mode):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_lists = patient_lists\n",
    "        self.mode = mode\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_with_aspect_ratio(keys, target_size):\n",
    "        def transform(data):\n",
    "            for key in keys:\n",
    "                volume = data[key]\n",
    "                original_shape = volume.shape[-3:]\n",
    "    \n",
    "                scaling_factor = min(\n",
    "                    target_size[0] / original_shape[0],\n",
    "                    target_size[1] / original_shape[1],\n",
    "                    target_size[2] / original_shape[2]\n",
    "                )\n",
    "    \n",
    "                # Computing the intermediate size while preserving aspect ratio\n",
    "                new_shape = [\n",
    "                    int(dim * scaling_factor) for dim in original_shape\n",
    "                ]\n",
    "    \n",
    "                # Resizing to the intermediate shape\n",
    "                resize_transform = Resized(keys=[key], spatial_size=new_shape, mode=\"trilinear\" if key == \"imgs\" else \"nearest-exact\")\n",
    "                data = resize_transform(data)\n",
    "    \n",
    "                # Padding to the final target size\n",
    "                pad_transform = SpatialPadd(keys=[key], spatial_size=target_size, mode=\"constant\")\n",
    "                data = pad_transform(data)\n",
    "            return data\n",
    "\n",
    "        return transform\n",
    "    \n",
    "    def preprocess(cls, data, mode):\n",
    "        if mode == 'training':\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True),\n",
    "              \n",
    "            RandFlipd(keys=[\"imgs\", \"masks\"],   \n",
    "                    prob=0.5,                 \n",
    "                    spatial_axis=2,  \n",
    "            ),\n",
    "\n",
    "            RandAdjustContrastd(\n",
    "                keys=[\"imgs\"],          \n",
    "                prob=0.15,             \n",
    "                gamma=(0.65, 1.5),   \n",
    "            ),\n",
    "            \n",
    "        ])\n",
    "\n",
    "        elif mode == 'validation':\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True)\n",
    "\n",
    "        ])\n",
    "\n",
    "        else: # 'testing'\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True)\n",
    "\n",
    "        ])\n",
    "\n",
    "        augmented_data = transform(data)\n",
    "        return augmented_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_lists[idx]\n",
    "        loadimage = LoadImage(reader='NibabelReader', image_only=True)\n",
    "\n",
    "        patient_folder_path = os.path.join(self.data_dir, patient_id)\n",
    "\n",
    "        def resolve_file_path(folder, name):\n",
    "            file_path = os.path.join(folder, name)\n",
    "            # Check if the given path is a directory (case with 4 subdirs)\n",
    "            if os.path.isdir(file_path):\n",
    "                # Find the first file inside the directory that ends with .nii\n",
    "                for root, _, files in os.walk(file_path):\n",
    "                    for file in files:\n",
    "                        if file.endswith(\".nii\"):\n",
    "                            return os.path.join(root, file)\n",
    "            return file_path\n",
    "\n",
    "\n",
    "        # Resolve paths for all required image types\n",
    "        t1c_path  = resolve_file_path(patient_folder_path, patient_id + '-t1c.nii')\n",
    "        t1n_path  = resolve_file_path(patient_folder_path, patient_id + '-t1n.nii')\n",
    "        t2f_path  = resolve_file_path(patient_folder_path, patient_id + '-t2f.nii')\n",
    "        t2w_path  = resolve_file_path(patient_folder_path, patient_id + '-t2w.nii')\n",
    "        seg_path  = os.path.join(patient_folder_path, patient_id + '-seg.nii')\n",
    "\n",
    "        t1c_loader   = loadimage( t1c_path )\n",
    "        t1n_loader   = loadimage( t1n_path )\n",
    "        t2f_loader   = loadimage( t2f_path )\n",
    "        t2w_loader   = loadimage( t2w_path )\n",
    "        masks_loader = loadimage( seg_path )\n",
    "\n",
    "        # Make the dimension of channel\n",
    "        t1c_tensor   = torch.Tensor(t1c_loader).unsqueeze(0)\n",
    "        t1n_tensor   = torch.Tensor(t1n_loader).unsqueeze(0)\n",
    "        t2f_tensor   = torch.Tensor(t2f_loader).unsqueeze(0)\n",
    "        t2w_tensor   = torch.Tensor(t2w_loader).unsqueeze(0)\n",
    "        masks_tensor = torch.Tensor(masks_loader).unsqueeze(0)\n",
    "\n",
    "        # dividing the mask to each class\n",
    "        '''\n",
    "        Output channel\n",
    "        [1, 0, 0, 0] = BackGround (BG)\n",
    "        [0, 1, 0, 0] = Necrosis (NE)\n",
    "        [0, 0, 1, 0] = Edema (ED)\n",
    "        [0, 0, 0, 1] = Enhancing Tumor (ET)\n",
    "        \n",
    "        '''\n",
    "\n",
    "        # x = torch.ones( masks_tensor.size() )\n",
    "        # y = torch.zeros( masks_tensor.size() )\n",
    "\n",
    "        # masks_BG = torch.where( masks_tensor == 0, x, y )\n",
    "        # masks_NE = torch.where( masks_tensor == 1, x, y )\n",
    "        # masks_ED = torch.where( masks_tensor == 2, x, y )\n",
    "        # masks_ET = torch.where( masks_tensor == 3, x, y )\n",
    "\n",
    "        concat_tensor = torch.cat( (t1c_tensor, t1n_tensor, t2f_tensor, t2w_tensor, masks_tensor), 0 )\n",
    "\n",
    "        data = {\n",
    "            \n",
    "            'imgs'  : np.array(concat_tensor[0:4,:,:,:]),\n",
    "            'masks' : np.array(concat_tensor[4:,:,:,:])\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "        augmented_imgs_masks = self.preprocess(data, self.mode)\n",
    "        imgs  = np.array(augmented_imgs_masks['imgs'])\n",
    "        masks = np.array(augmented_imgs_masks['masks'])\n",
    "\n",
    "        y = {\n",
    "\n",
    "            'imgs'  : torch.from_numpy(imgs).type(torch.FloatTensor),\n",
    "            'masks' : torch.from_numpy(masks).type(torch.FloatTensor),\n",
    "            'patient_id' : patient_id\n",
    "\n",
    "        }\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.219125Z",
     "iopub.status.busy": "2024-11-29T22:28:29.218867Z",
     "iopub.status.idle": "2024-11-29T22:28:29.260155Z",
     "shell.execute_reply": "2024-11-29T22:28:29.259287Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.219101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders(args):\n",
    "    random.seed(5)\n",
    "    split_ratio = {'training': 0.7, 'validation': 0.1, 'testing': 0.2}\n",
    "    data_dir = args['data_dir']\n",
    "    \n",
    "    patient_lists = os.listdir( data_dir )\n",
    "    patient_lists.sort()\n",
    "    total_patients = len(patient_lists)\n",
    "    \n",
    "    # Shuffle the patient list\n",
    "    random.shuffle(patient_lists)\n",
    "    \n",
    "    train_split = int(split_ratio['training'] * total_patients)\n",
    "    val_split = int(split_ratio['validation'] * total_patients)\n",
    "    \n",
    "    train_patient_lists = patient_lists[:train_split]\n",
    "    val_patient_lists = patient_lists[train_split : train_split + val_split]\n",
    "    test_patient_lists = patient_lists[train_split + val_split :]\n",
    "    \n",
    "    train_patient_lists.sort()\n",
    "    val_patient_lists.sort()\n",
    "    test_patient_lists.sort()\n",
    "    \n",
    "    print(f'Number of training samples', len(train_patient_lists))\n",
    "    print(f'Number of validation samples', len(val_patient_lists))\n",
    "    print(f'Number of testing samples', len(test_patient_lists))\n",
    "\n",
    "    trainDataset = CustomDataset3D( data_dir, train_patient_lists, mode='training')\n",
    "    valDataset   = CustomDataset3D( data_dir, val_patient_lists, mode='validation')\n",
    "    testDataset  = CustomDataset3D( data_dir, test_patient_lists, mode='testing')\n",
    "    \n",
    "    trainLoader = DataLoader(\n",
    "        trainDataset, batch_size=args['train_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=True)\n",
    "    \n",
    "    valLoader = DataLoader(\n",
    "        valDataset, batch_size=args['val_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=False)\n",
    "    \n",
    "    testLoader = DataLoader(\n",
    "        testDataset, batch_size=args['test_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=False)\n",
    "\n",
    "    return trainLoader, valLoader, testLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.289534Z",
     "iopub.status.busy": "2024-11-29T22:28:29.289182Z",
     "iopub.status.idle": "2024-11-29T22:28:29.301767Z",
     "shell.execute_reply": "2024-11-29T22:28:29.301125Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.289499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'workers': 2,\n",
    "#     'epochs': 10,\n",
    "#     'train_batch_size': 2,\n",
    "#     'val_batch_size': 2,\n",
    "#     'test_batch_size': 2,\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'weight_decay': 1e-5,\n",
    "#     'lambd': 0.0051,\n",
    "#     'data_dir': '/kaggle/input/bratsglioma/Training/',\n",
    "#     'in_checkpoint_dir': Path('/kaggle/input/adultgliomamodel-45epochs'),\n",
    "#     'out_checkpoint_dir': Path('/kaggle/working/')\n",
    "# }\n",
    "\n",
    "# trainLoader, valLoader, testLoader = prepare_data_loaders(args)\n",
    "\n",
    "# for step, y in enumerate( trainLoader ):\n",
    "#   print(y['imgs'].shape)\n",
    "#   print(y['masks'].shape)\n",
    "#   print(y['patient_id'])\n",
    "\n",
    "#   fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "#   for sequence in range(4):\n",
    "#     sequence_data = y['imgs'][0][sequence, :, :, :].cpu().detach().numpy()\n",
    "#     slice_index = sequence_data.shape[2] // 2\n",
    "#     axes[sequence].imshow(sequence_data[:, :, slice_index], cmap='gray', origin='lower')\n",
    "#     axes[sequence].set_title(f'Sequence {sequence + 1}')\n",
    "\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynUNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.303213Z",
     "iopub.status.busy": "2024-11-29T22:28:29.302897Z",
     "iopub.status.idle": "2024-11-29T22:28:29.324937Z",
     "shell.execute_reply": "2024-11-29T22:28:29.324338Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.303178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnetBasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN module module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        stride: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            dropout=dropout,\n",
    "            conv_only=True,\n",
    "        )\n",
    "\n",
    "        self.conv2 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            conv_only=True\n",
    "        )\n",
    "        self.lrelu = get_act_layer(name=act_name)\n",
    "        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        out = self.norm1(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UnetUpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "        trans_bias: transposed convolution bias.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        upsample_kernel_size: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "        trans_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        upsample_stride = upsample_kernel_size\n",
    "        \n",
    "        # ( a purple arrow in the paper )\n",
    "        self.transp_conv = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=upsample_kernel_size,\n",
    "            stride=upsample_stride,\n",
    "            dropout=dropout,\n",
    "            bias=trans_bias,\n",
    "            conv_only=True,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "        \n",
    "        # A light blue conv blocks in the decoder of nnUNet\n",
    "        self.conv_block = UnetBasicBlock(\n",
    "            spatial_dims,\n",
    "            out_channels + out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            norm_name=norm_name,\n",
    "            act_name=act_name,\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, skip):\n",
    "        # number of channels for skip should equals to out_channels\n",
    "        out = self.transp_conv(inp)\n",
    "        out = torch.cat((out, skip), dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UnetOutBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, in_channels: int, out_channels: int, dropout: Optional[Union[Tuple, str, float]] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = get_conv_layer(\n",
    "            spatial_dims, in_channels, out_channels, kernel_size=1, stride=1, dropout=dropout, bias=True, conv_only=True\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.conv(inp)\n",
    "    \n",
    "\n",
    "def get_conv_layer(\n",
    "    spatial_dims: int,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: Union[Sequence[int], int] = 3,\n",
    "    stride: Union[Sequence[int], int] = 1,\n",
    "    act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
    "    norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "    dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    bias: bool = False,\n",
    "    conv_only: bool = True,\n",
    "    is_transposed: bool = False,\n",
    "):\n",
    "    padding = get_padding(kernel_size, stride)\n",
    "    output_padding = None\n",
    "    if is_transposed:\n",
    "        output_padding = get_output_padding(kernel_size, stride, padding)\n",
    "    \n",
    "    return Convolution(\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        strides=stride,\n",
    "        kernel_size=kernel_size,\n",
    "        act=act,\n",
    "        norm=norm,\n",
    "        dropout=dropout,\n",
    "        bias=bias,\n",
    "        conv_only=conv_only,\n",
    "        is_transposed=is_transposed,\n",
    "        padding=padding,\n",
    "        output_padding=output_padding,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = (kernel_size_np - stride_np + 1) / 2\n",
    "    if np.min(padding_np) < 0:\n",
    "        raise AssertionError(\"padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    padding = tuple(int(p) for p in padding_np)\n",
    "\n",
    "    return padding if len(padding) > 1 else padding[0]\n",
    "\n",
    "\n",
    "def get_output_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = np.atleast_1d(padding)\n",
    "\n",
    "    out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
    "    if np.min(out_padding_np) < 0:\n",
    "        raise AssertionError(\"out_padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    out_padding = tuple(int(p) for p in out_padding_np)\n",
    "\n",
    "    return out_padding if len(out_padding) > 1 else out_padding[0]\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.326782Z",
     "iopub.status.busy": "2024-11-29T22:28:29.326352Z",
     "iopub.status.idle": "2024-11-29T22:28:29.343139Z",
     "shell.execute_reply": "2024-11-29T22:28:29.342506Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.326746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DynUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        deep_supervision: bool,\n",
    "        KD: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.KD_enabled = KD\n",
    "        \n",
    "        self.input_conv = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=self.in_channels,\n",
    "                                     out_channels=64,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1\n",
    "                                     )\n",
    "        self.down1 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=64,\n",
    "                                     out_channels=96,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2 # Reduces spatial dims by 2\n",
    "                                     )\n",
    "        self.down2 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=96,\n",
    "                                     out_channels=128,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down3 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=128,\n",
    "                                     out_channels=192,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down4 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=192,\n",
    "                                     out_channels=256,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down5 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=256,\n",
    "                                     out_channels=384,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.bottleneck = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=384,\n",
    "                                     out_channels=512,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.up1 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=512,\n",
    "                                out_channels=384,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up2 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=384,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up3 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=256,\n",
    "                                out_channels=192,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up4 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=192,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        \n",
    "        self.up5 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=128,\n",
    "                                out_channels=96,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )        \n",
    "        self.up6 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=96,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.out1 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=64,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out2 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=96,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out3 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=128,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        \n",
    "    def forward( self, input ):\n",
    "        \n",
    "        # Input\n",
    "        x0 = self.input_conv( input ) # x0.shape = (B x 64 x 128 x 128 x 128)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.down1( x0 ) # x1.shape = (B x 96 x 64 x 64 x 64) \n",
    "        x2 = self.down2( x1 ) # x2.shape = (B x 128 x 32 x 32 x 32)\n",
    "        x3 = self.down3( x2 ) # x3.shape = (B x 192 x 16 x 16 x 16)\n",
    "        x4 = self.down4( x3 ) # x4.shape = (B x 256 x 8 x 8 x 8)   \n",
    "        x5 = self.down5( x4 ) # x5.shape = (B x 384 x 4 x 4 x 4)   \n",
    "        \n",
    "        # Bottleneck\n",
    "        x6 = self.bottleneck( x5 ) # x6.shape = (B x 512 x 2 x 2 x 2)\n",
    "        \n",
    "        # Decoder\n",
    "        x7  = self.up1( x6, x5 )  # x7.shape  = (B x 384 x 4 x 4 x 4)\n",
    "        x8  = self.up2( x7, x4 )  # x8.shape  = (B x 256 x 8 x 8 x 8)\n",
    "        x9  = self.up3( x8, x3 )  # x9.shape  = (B x 192 x 16 x 16 x 16)\n",
    "        x10 = self.up4( x9, x2 )  # x10.shape = (B x 128 x 32 x 32 x 32)\n",
    "        x11 = self.up5( x10, x1 ) # x11.shape = (B x 96 x 64 x 64 x 64)\n",
    "        x12 = self.up6( x11, x0 ) # x12.shape = (B x 64 x 128 x 128 x 128)\n",
    "        \n",
    "        # Output\n",
    "        output1 = self.out1( x12 )\n",
    "        \n",
    "        if (self.training and self.deep_supervision) or self.KD_enabled:\n",
    "            \n",
    "            # output['pred'].shape = B x 3 x 4 x 128 x 128 x 128\n",
    "            output2 = interpolate( self.out2( x11 ), output1.shape[2:])\n",
    "            output3 = interpolate( self.out3( x10 ), output1.shape[2:])\n",
    "            output_all = [ output1, output2, output3 ]\n",
    "            return { 'pred' : torch.stack(output_all, dim=1),\n",
    "                     'bottleneck_feature_map' : x6 }\n",
    "        \n",
    "        return { 'pred' : output1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.344325Z",
     "iopub.status.busy": "2024-11-29T22:28:29.344066Z",
     "iopub.status.idle": "2024-11-29T22:28:29.356175Z",
     "shell.execute_reply": "2024-11-29T22:28:29.355506Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.344289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "# from torchsummary import summary\n",
    "\n",
    "# # Initialize your DynUNet model\n",
    "# model = DynUNet(spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=True, KD=True)\n",
    "\n",
    "# # Print model summary\n",
    "# summary(model, input_size=(4, 128, 128, 128))  # Adjust input_size according to your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:29.358948Z",
     "iopub.status.busy": "2024-11-29T22:28:29.358703Z",
     "iopub.status.idle": "2024-11-29T22:28:40.202237Z",
     "shell.execute_reply": "2024-11-29T22:28:40.20129Z",
     "shell.execute_reply.started": "2024-11-29T22:28:29.358925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install clearml\n",
    "from clearml import Task\n",
    "\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=CLEARML_API_ACCESS_KEY  \n",
    "%env CLEARML_API_SECRET_KEY=CLEARML_API_SECRET_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.209905Z",
     "iopub.status.busy": "2024-11-29T22:28:40.209647Z",
     "iopub.status.idle": "2024-11-29T22:28:40.225499Z",
     "shell.execute_reply": "2024-11-29T22:28:40.224705Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.209879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.dice = DiceLoss(sigmoid=True, batch=True, smooth_nr=1e-05, smooth_dr=1e-05)\n",
    "        self.ce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def _loss(self, p, y):\n",
    "        return self.dice(p, y) + self.ce(p, y.float())\n",
    "\n",
    "    def forward(self, p, y):\n",
    "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
    "        p_wt, p_tc, p_et = p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1), p[:, 3].unsqueeze(1)\n",
    "        l_wt, l_tc, l_et = self._loss(p_wt, y_wt), self._loss(p_tc, y_tc), self._loss(p_et, y_et)\n",
    "        return l_wt + l_tc + l_et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.22665Z",
     "iopub.status.busy": "2024-11-29T22:28:40.226342Z",
     "iopub.status.idle": "2024-11-29T22:28:40.241089Z",
     "shell.execute_reply": "2024-11-29T22:28:40.24031Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.226625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, epoch): # teacher_model, valLoader, epoch\n",
    "    torch.manual_seed(0)\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = LossFunction()\n",
    "    n_val_batches = len(loader)\n",
    "    running_loss = 0\n",
    "\n",
    "    with tqdm(total=n_val_batches, desc='Validating', unit='batch', leave=False) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for step, y in enumerate(loader):\n",
    "                y['imgs'], y['masks']= y['imgs'].to('cuda'), y['masks'].to('cuda')\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(y['imgs']) # No need now for KD!\n",
    "                    val_loss = loss_fn( output['pred'], y['masks'] ) #output['pred'] has 'pred' only due to model.eval() so self.training = False\n",
    "                    print(f\"Validation loss per batch: {val_loss}\")\n",
    "                    \n",
    "                running_loss += val_loss\n",
    "                print(f\"Cumulative loss: {running_loss}\")            \n",
    "                pbar.update(1)\n",
    "    \n",
    "            epoch_loss = running_loss / n_val_batches\n",
    "            print(f\"------Final validation loss after epoch {epoch + 1}: {epoch_loss}-------------\")\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.242557Z",
     "iopub.status.busy": "2024-11-29T22:28:40.242253Z",
     "iopub.status.idle": "2024-11-29T22:28:40.259484Z",
     "shell.execute_reply": "2024-11-29T22:28:40.25877Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.242508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_model(trainLoader, valLoader, args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    args['out_checkpoint_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    task = Task.init(project_name=\"AdultGliomaModelSegmentation\", task_name=f\"Finalized Adult Glioma Segmentation _ Epochs AdamW\", reuse_last_task_id=True)\n",
    "    task.connect(args)\n",
    "\n",
    "    loss_fn = LossFunction()\n",
    "    teacher_model = DynUNet(spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=True).to(device)\n",
    "    optimizer_teacher = optim.AdamW(teacher_model.parameters(), lr=args['learning_rate'], weight_decay=args['weight_decay'], eps=1e-4)\n",
    "    # optimizer_teacher = optim.SGD(teacher_model.parameters(), lr=args['learning_rate'], weight_decay=args['weight_decay'],\n",
    "    #                                     momentum=0.99, nesterov=True)\n",
    "    scaler = amp.GradScaler('cuda')\n",
    "    n_train_batches = len(trainLoader)\n",
    "    scheduler_teacher = optim.lr_scheduler.ReduceLROnPlateau(optimizer_teacher, mode='min', factor=0.1, patience=6, cooldown=1, threshold=0.001, min_lr=1e-6)\n",
    "    # scheduler_teacher = PolyLRScheduler(optimizer_teacher, initial_lr=args['learning_rate'], max_steps=120)\n",
    "\n",
    "    teacher_epoch_checkpoint = f'Teacher_model_after_epoch_75_trainLoss_0.6123_valLoss_0.2997.pth'\n",
    "    if (args['in_checkpoint_dir'] / teacher_epoch_checkpoint).is_file():\n",
    "        print(f\"Found model {teacher_epoch_checkpoint}\")\n",
    "        ckpt = torch.load(args['in_checkpoint_dir'] / teacher_epoch_checkpoint, map_location='cuda', weights_only=True)\n",
    "        start_epoch = ckpt['epoch'] + 1 # To start after last saved epoch\n",
    "        teacher_model.load_state_dict(ckpt['teacher_model'])\n",
    "        optimizer_teacher.load_state_dict(ckpt['optimizer_teacher'])\n",
    "        scaler.load_state_dict(ckpt['grad_scaler_state'])\n",
    "        scheduler_teacher.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        # optimizer_teacher.param_groups[0]['lr'] = args['learning_rate'] # Leave this (overwrite) or remove it as you wish\n",
    "        print(f\"Loaded model {teacher_epoch_checkpoint} with lr: {optimizer_teacher.param_groups[0]['lr']}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    print(f'''Starting training:\n",
    "            Epochs:          From {start_epoch + 1} to {start_epoch + args['epochs']}\n",
    "            Batch size:      {args['train_batch_size']}\n",
    "            Learning rate:   {optimizer_teacher.param_groups[0]['lr']}\n",
    "            Training data coming from:   {args['data_dir']}\n",
    "    ''')\n",
    "\n",
    "    # Starting training\n",
    "    for epoch in range(start_epoch, start_epoch + args['epochs']):\n",
    "        teacher_model.train()\n",
    "        epoch_loss_teacher = 0\n",
    "        \n",
    "        with tqdm(total=n_train_batches, desc=f\"(Epoch {epoch + 1}/{start_epoch + args['epochs']})\", unit='batch') as pbar:\n",
    "            for step, y in enumerate(trainLoader):\n",
    "                y['imgs'], y['masks']= y['imgs'].to(device), y['masks'].to(device)\n",
    "\n",
    "                set_requires_grad(teacher_model, requires_grad=True)\n",
    "                optimizer_teacher.zero_grad()\n",
    "                with amp.autocast('cuda'):  # Mixed precision\n",
    "                    teacher_output = teacher_model(y['imgs'])\n",
    "                    loss_teacher_decoder_1 = loss_fn(teacher_output['pred'][:, 0], y['masks']) # B x 3 x 4 x 128 x 128 x 128\n",
    "                    loss_teacher_decoder_2 = loss_fn(teacher_output['pred'][:, 1], y['masks'])\n",
    "                    loss_teacher_decoder_3 = loss_fn(teacher_output['pred'][:, 2], y['masks'])\n",
    "\n",
    "                    print(f'loss_t_decoder1/batch : {loss_teacher_decoder_1:.4f} || loss_t_decoder2/batch : {loss_teacher_decoder_2:.4f} || loss_t_decoder3/batch : {loss_teacher_decoder_3:.4f}')\n",
    "                    batch_loss_teacher = loss_teacher_decoder_1 + (0.5 * loss_teacher_decoder_2) + (0.25 * loss_teacher_decoder_3)\n",
    "\n",
    "                scaler.scale(batch_loss_teacher).backward()\n",
    "                scaler.step(optimizer_teacher)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss_teacher += batch_loss_teacher.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "            epoch_loss_teacher /= n_train_batches\n",
    "            print(f\"------Final training loss after epoch {epoch + 1}: {epoch_loss_teacher}-------------\")\n",
    "\n",
    "            task.get_logger().report_scalar(\"Loss\", \"train_loss\", iteration=epoch+1, value=epoch_loss_teacher)\n",
    "            task.get_logger().report_scalar(\"LR\", \"learning_rate\", iteration=epoch+1, value=optimizer_teacher.param_groups[0]['lr'])\n",
    "\n",
    "        # Validation phase\n",
    "        print(f'''\n",
    "            Starting validation:\n",
    "            Epoch:           {epoch + 1}\n",
    "            Batch size:      {args['val_batch_size']}\n",
    "            Validation data coming from: {args['data_dir']}\n",
    "\n",
    "        ''')\n",
    "\n",
    "        val_loss = evaluate(teacher_model, valLoader, epoch)\n",
    "        scheduler_teacher.step(val_loss)\n",
    "        print(f\"Learning rate after epoch {epoch + 1}: {optimizer_teacher.param_groups[0]['lr']}\")\n",
    "        task.get_logger().report_scalar(\"Loss\", \"val_loss\", iteration=epoch+1, value=val_loss)\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'teacher_model': teacher_model.state_dict(),\n",
    "            'optimizer_teacher': optimizer_teacher.state_dict(),\n",
    "            'lr': optimizer_teacher.param_groups[0]['lr'],\n",
    "            'grad_scaler_state': scaler.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler_teacher.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "\n",
    "        }\n",
    "\n",
    "        torch.save(state, args['out_checkpoint_dir'] / f'Teacher_model_after_epoch_{epoch + 1}_trainLoss_{epoch_loss_teacher:.4f}_valLoss_{val_loss:.4f}.pth')\n",
    "        print(f\"Model saved after epoch {epoch + 1}\")\n",
    "        # task.upload_artifact(name=f\"model_checkpoint_epoch_{epoch + 1}\", artifact_object=checkpoint_path)\n",
    "\n",
    "    task.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.260737Z",
     "iopub.status.busy": "2024-11-29T22:28:40.260422Z",
     "iopub.status.idle": "2024-11-29T22:28:40.275038Z",
     "shell.execute_reply": "2024-11-29T22:28:40.274247Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.2607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 25,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 3,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'data_dir': '/kaggle/input/bratsglioma/Training/',\n",
    "    'in_checkpoint_dir': Path('/kaggle/input/gliomateachernormalizednew/'),\n",
    "    'out_checkpoint_dir': Path('/kaggle/working/')\n",
    "}\n",
    "trainLoader, valLoader, testLoader = prepare_data_loaders(args)\n",
    "run_model(trainLoader, valLoader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.276323Z",
     "iopub.status.busy": "2024-11-29T22:28:40.275952Z",
     "iopub.status.idle": "2024-11-29T22:28:40.288043Z",
     "shell.execute_reply": "2024-11-29T22:28:40.287211Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.276287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # For freeing gpu\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.299724Z",
     "iopub.status.busy": "2024-11-29T22:28:40.299513Z",
     "iopub.status.idle": "2024-11-29T22:28:40.314359Z",
     "shell.execute_reply": "2024-11-29T22:28:40.313603Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.299703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "\n",
    "class ComputeMetrics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComputeMetrics, self).__init__()\n",
    "        self.dice_metric = DiceMetric(reduction=\"mean_batch\")\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(percentile=95.0, reduction=\"mean_batch\")\n",
    "\n",
    "    def compute(self, p, y, lbl):\n",
    "        self.dice_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "        \n",
    "        print(f\"{lbl} - Prediction unique values: {torch.unique(p)}\")\n",
    "        print(f\"{lbl} - Ground truth unique values: {torch.unique(y)}\")\n",
    "\n",
    "        if torch.sum(y.float()) == 0 and torch.sum(p.float()) == 0:  # True Negative Case: No foreground pixels in GT\n",
    "            print(f\"{lbl} - No positive samples in ground truth.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {1.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {0.0}\")\n",
    "            return torch.tensor(1.0), torch.tensor(0.0)\n",
    "        \n",
    "        if torch.sum(p.float()) == 0 and torch.sum(y.float()) > 0:  # False Negative Case: GT has 1s, Prediction is all 0s\n",
    "            print(f\"{lbl} - False Negative Case: GT has positive samples, but prediction is empty.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {0.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {373.1287}\")\n",
    "            return torch.tensor(0.0), torch.tensor(373.1287)\n",
    "        \n",
    "        if torch.sum(p.float()) > 0 and torch.sum(y.float()) == 0:  # False Positive Case: Prediction has 1s, GT is all 0s\n",
    "            print(f\"{lbl} - False Positive Case: Prediction has positives, but ground truth is empty.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {0.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {373.1287}\")\n",
    "            return torch.tensor(0.0), torch.tensor(373.1287)\n",
    "\n",
    "        # Compute metrics normally\n",
    "        dice_score = self.dice_metric(p.float(), y.float())\n",
    "        hausdorff_dist = self.hausdorff_metric(p.float(), y.float())\n",
    "\n",
    "        print(f\"Dice scores for {lbl} for this batch:\\n {dice_score.item()}\")\n",
    "        print(f\"Hausdorff distances for {lbl} for this batch:\\n{hausdorff_dist.item()}\")\n",
    "    \n",
    "        return dice_score, hausdorff_dist\n",
    "\n",
    "    def forward(self, p, y):\n",
    "        p = (torch.sigmoid(p) > 0.5)\n",
    "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
    "        p_wt, p_tc, p_et = p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1), p[:, 3].unsqueeze(1)\n",
    "        \n",
    "        dice_wt, hd_wt = self.compute(p_wt, y_wt, 'wt')\n",
    "        dice_tc, hd_tc = self.compute(p_tc, y_tc, 'tc')\n",
    "        dice_et, hd_et = self.compute(p_et, y_et, 'et')\n",
    "        \n",
    "        return [dice_wt, hd_wt], [dice_tc, hd_tc], [dice_et, hd_et]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T22:28:40.31606Z",
     "iopub.status.busy": "2024-11-29T22:28:40.315753Z",
     "iopub.status.idle": "2024-11-29T22:28:40.328529Z",
     "shell.execute_reply": "2024-11-29T22:28:40.327762Z",
     "shell.execute_reply.started": "2024-11-29T22:28:40.316025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_net(model, loader): # student_model, testLoader\n",
    "    torch.manual_seed(0)\n",
    "    model.eval()\n",
    "    n_test_batches = len(loader)\n",
    "\n",
    "    compute_metrics = ComputeMetrics()\n",
    "    total_metrics = {\"WT\": {'dice_score': 0, 'hausdorff_distance': 0},\n",
    "                     \"TC\": {'dice_score': 0, 'hausdorff_distance': 0},\n",
    "                     \"ET\": {'dice_score': 0, 'hausdorff_distance': 0}}\n",
    "\n",
    "    with tqdm(total=n_test_batches, desc='Testing', unit='batch', leave=False) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for step, y in enumerate(loader):\n",
    "                y['imgs'], y['masks']= y['imgs'].to('cuda'), y['masks'].to('cuda')\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    print(\"--------Now patient:\", y['patient_id'])\n",
    "                    output = model(y['imgs'])\n",
    "                    wt_metrics, tc_metrics, et_metrics = compute_metrics(output['pred'], y['masks'])\n",
    "                    \n",
    "                    total_metrics['WT']['dice_score'] += wt_metrics[0].item()\n",
    "                    total_metrics['WT']['hausdorff_distance'] += wt_metrics[1].item()\n",
    "\n",
    "                    total_metrics['TC']['dice_score'] += tc_metrics[0].item()\n",
    "                    total_metrics['TC']['hausdorff_distance'] += tc_metrics[1].item()\n",
    "\n",
    "                    total_metrics['ET']['dice_score'] += et_metrics[0].item()\n",
    "                    total_metrics['ET']['hausdorff_distance'] += et_metrics[1].item()\n",
    "                                    \n",
    "                pbar.update(1)\n",
    "\n",
    "        total_metrics['WT']['dice_score'] /= n_test_batches\n",
    "        total_metrics['WT']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "        total_metrics['TC']['dice_score'] /= n_test_batches\n",
    "        total_metrics['TC']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "        total_metrics['ET']['dice_score'] /= n_test_batches\n",
    "        total_metrics['ET']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "\n",
    "        print(\"************************************************************************\")\n",
    "        print(f\"Average Dice Score for WT: {total_metrics['WT']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for WT: {total_metrics['WT']['hausdorff_distance']:.4f}\")\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "                                     \n",
    "        print(f\"Average Dice Score for TC: {total_metrics['TC']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for TC: {total_metrics['TC']['hausdorff_distance']:.4f}\")\n",
    "                              \n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "                                     \n",
    "        print(f\"Average Dice Score for ET: {total_metrics['ET']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for ET: {total_metrics['ET']['hausdorff_distance']:.4f}\")\n",
    "        print(\"************************************************************************\")\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'data_dir': '/kaggle/input/bratsglioma/Training/',\n",
    "    'in_checkpoint_dir': Path('/kaggle/input/glioma75epochs-redlr'),\n",
    "    'out_checkpoint_dir': Path('/kaggle/working/')\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "teacher_path = Path(f'/kaggle/input/gliomateachernewlabels/Teacher_model_after_epoch_100_trainLoss_0.5972_valLoss_0.3019.pth')\n",
    "teacher_model = DynUNet(spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "\n",
    "if (teacher_path).is_file():\n",
    "    print(f\"Found model: {teacher_path}\")\n",
    "    ckpt = torch.load(teacher_path, map_location='cuda', weights_only=True)\n",
    "    teacher_model.load_state_dict(ckpt['teacher_model'])\n",
    "    print(f\"Loaded model: {teacher_path}\")\n",
    "    test_net(teacher_model, testLoader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5929819,
     "sourceId": 9697878,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6222947,
     "sourceId": 10102734,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
