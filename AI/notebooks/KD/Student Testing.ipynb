{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:43:50.934889Z",
     "iopub.status.busy": "2025-03-08T21:43:50.933871Z",
     "iopub.status.idle": "2025-03-08T21:44:43.049718Z",
     "shell.execute_reply": "2025-03-08T21:44:43.048761Z",
     "shell.execute_reply.started": "2025-03-08T21:43:50.93483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install monai\n",
    "\n",
    "import nibabel as nib\n",
    "from monai.transforms import LoadImage, Compose, NormalizeIntensityd, RandSpatialCropd, RandFlipd, \\\n",
    "                             RandRotate90d, Rand3DElasticd, RandAdjustContrastd, CenterSpatialCropd,\\\n",
    "                             Resized, RandRotated, RandZoomd, RandGaussianNoised, Spacingd, RandShiftIntensityd,  CropForegroundd, SpatialPadd, AsDiscrete, GridPatchd\\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate\n",
    "import pdb\n",
    "\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "from monai.networks import one_hot\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from grpc import insecure_channel\n",
    "import argparse\n",
    "from torch import optim, amp\n",
    "from monai.losses import DiceLoss,BarlowTwinsLoss\n",
    "import torch.distributed as dist\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pdb\n",
    "import logging\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:44:43.052075Z",
     "iopub.status.busy": "2025-03-08T21:44:43.051493Z",
     "iopub.status.idle": "2025-03-08T21:44:43.069522Z",
     "shell.execute_reply": "2025-03-08T21:44:43.068648Z",
     "shell.execute_reply.started": "2025-03-08T21:44:43.052046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset3D(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dirs,\n",
    "        patient_lists,\n",
    "        mode\n",
    "        ):\n",
    "\n",
    "        self.data_dirs = data_dirs\n",
    "        self.patient_lists = patient_lists\n",
    "        self.mode = mode\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_with_aspect_ratio(keys, target_size):\n",
    "        def transform(data):\n",
    "            for key in keys:\n",
    "                volume = data[key]\n",
    "                original_shape = volume.shape[-3:]\n",
    "    \n",
    "                scaling_factor = min(\n",
    "                    target_size[0] / original_shape[0],\n",
    "                    target_size[1] / original_shape[1],\n",
    "                    target_size[2] / original_shape[2]\n",
    "                )\n",
    "    \n",
    "                # Computing the intermediate size while preserving aspect ratio\n",
    "                new_shape = [\n",
    "                    int(dim * scaling_factor) for dim in original_shape\n",
    "                ]\n",
    "    \n",
    "                # Resizing to the intermediate shape\n",
    "                resize_transform = Resized(keys=[key], spatial_size=new_shape, mode=\"trilinear\" if key == \"imgs\" else \"nearest-exact\")\n",
    "                data = resize_transform(data)\n",
    "    \n",
    "                # Padding to the final target size\n",
    "                pad_transform = SpatialPadd(keys=[key], spatial_size=target_size, mode=\"constant\")\n",
    "                data = pad_transform(data)\n",
    "            return data\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def preprocess(cls, data, mode):\n",
    "        if mode == 'training':\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True),\n",
    "              \n",
    "            RandFlipd(keys=[\"imgs\", \"masks\"],   \n",
    "                    prob=0.5,                 \n",
    "                    spatial_axis=2,  \n",
    "            ),\n",
    "\n",
    "            RandAdjustContrastd(\n",
    "                keys=[\"imgs\"],          \n",
    "                prob=0.15,             \n",
    "                gamma=(0.65, 1.5),   \n",
    "            ),\n",
    "            \n",
    "        ])\n",
    "\n",
    "        elif mode == 'validation':\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True)\n",
    "\n",
    "        ])\n",
    "\n",
    "        else: # 'testing'\n",
    "          transform = Compose([\n",
    "            CropForegroundd(keys=[\"imgs\", \"masks\"], source_key=\"imgs\"),\n",
    "            cls.resize_with_aspect_ratio(keys=[\"imgs\", \"masks\"], target_size=[128, 128, 128]),\n",
    "            NormalizeIntensityd( keys=['imgs'], nonzero=False, channel_wise=True)\n",
    "\n",
    "        ])\n",
    "\n",
    "        augmented_data = transform(data)\n",
    "        return augmented_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patient_lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_lists[idx]\n",
    "        loadimage = LoadImage(reader='NibabelReader', image_only=True)\n",
    "\n",
    "        data_type=patient_id.split('-')[1]\n",
    "        if data_type == 'GLI':\n",
    "            patient_folder_path = os.path.join('/kaggle/input/bratsglioma/Training', patient_id)\n",
    "        elif data_type == 'SSA':\n",
    "            patient_folder_path = os.path.join('/kaggle/input/bratsafrica24', patient_id)\n",
    "        elif data_type == 'PED':\n",
    "            patient_folder_path = os.path.join('/kaggle/input/bratsped/Training', patient_id)\n",
    "        elif data_type == 'MEN':\n",
    "            patient_folder_path = os.path.join('/kaggle/input/bratsmen', patient_id)\n",
    "        else:\n",
    "            patient_folder_path = os.path.join('/kaggle/input/bratsmet24', patient_id)\n",
    "\n",
    "        def resolve_file_path(folder, name):\n",
    "            file_path = os.path.join(folder, name)\n",
    "            # Check if the given path is a directory (case with 4 subdirs)\n",
    "            if os.path.isdir(file_path):\n",
    "                # Find the first file inside the directory that ends with .nii\n",
    "                for root, _, files in os.walk(file_path):\n",
    "                    for file in files:\n",
    "                        if file.endswith(\".nii\"):\n",
    "                            return os.path.join(root, file)\n",
    "            return file_path\n",
    "\n",
    "\n",
    "        # Resolve paths for all required image types\n",
    "        t1c_path  = resolve_file_path(patient_folder_path, patient_id + '-t1c.nii')\n",
    "        t1n_path  = resolve_file_path(patient_folder_path, patient_id + '-t1n.nii')\n",
    "        t2f_path  = resolve_file_path(patient_folder_path, patient_id + '-t2f.nii')\n",
    "        t2w_path  = resolve_file_path(patient_folder_path, patient_id + '-t2w.nii')\n",
    "        seg_path  = os.path.join(patient_folder_path, patient_id + '-seg.nii')\n",
    "\n",
    "        t1c_loader   = loadimage( t1c_path )\n",
    "        t1n_loader   = loadimage( t1n_path )\n",
    "        t2f_loader   = loadimage( t2f_path )\n",
    "        t2w_loader   = loadimage( t2w_path )\n",
    "        masks_loader = loadimage( seg_path )\n",
    "\n",
    "        # Make the dimension of channel\n",
    "        t1c_tensor   = torch.Tensor(t1c_loader).unsqueeze(0)\n",
    "        t1n_tensor   = torch.Tensor(t1n_loader).unsqueeze(0)\n",
    "        t2f_tensor   = torch.Tensor(t2f_loader).unsqueeze(0)\n",
    "        t2w_tensor   = torch.Tensor(t2w_loader).unsqueeze(0)\n",
    "        masks_tensor = torch.Tensor(masks_loader).unsqueeze(0)\n",
    "\n",
    "        concat_tensor = torch.cat( (t1c_tensor, t1n_tensor, t2f_tensor, t2w_tensor, masks_tensor), 0 )\n",
    "        data = {            \n",
    "            'imgs'  : np.array(concat_tensor[0:4,:,:,:]),\n",
    "            'masks' : np.array(concat_tensor[4:,:,:,:])\n",
    "        }\n",
    "\n",
    "        augmented_imgs_masks = self.preprocess(data, self.mode)\n",
    "        imgs  = np.array(augmented_imgs_masks['imgs'])\n",
    "        masks = np.array(augmented_imgs_masks['masks'])\n",
    "\n",
    "        y = {\n",
    "\n",
    "            'imgs'  : torch.from_numpy(imgs).type(torch.FloatTensor),\n",
    "            'masks' : torch.from_numpy(masks).type(torch.FloatTensor),\n",
    "            'patient_id' : patient_id,\n",
    "            'data_type' : data_type\n",
    "\n",
    "        }\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:44:43.07081Z",
     "iopub.status.busy": "2025-03-08T21:44:43.070556Z",
     "iopub.status.idle": "2025-03-08T21:44:43.091389Z",
     "shell.execute_reply": "2025-03-08T21:44:43.090693Z",
     "shell.execute_reply.started": "2025-03-08T21:44:43.070786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_datasets(dataset_lists, batch_size=2):\n",
    "    min_len = min(len(dataset) for dataset in dataset_lists)\n",
    "    combined_paths = []\n",
    "    \n",
    "    for i in range(0, min_len, batch_size):\n",
    "        for dataset in dataset_lists:\n",
    "            batch = dataset[i:i + batch_size]\n",
    "            if len(batch)==batch_size:\n",
    "                combined_paths.extend(batch)\n",
    "            else: \n",
    "                break\n",
    "                \n",
    "    return combined_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:44:43.092918Z",
     "iopub.status.busy": "2025-03-08T21:44:43.092546Z",
     "iopub.status.idle": "2025-03-08T21:44:43.108749Z",
     "shell.execute_reply": "2025-03-08T21:44:43.10796Z",
     "shell.execute_reply.started": "2025-03-08T21:44:43.09287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders(args):\n",
    "    train_datasets, val_datasets, test_datasets = [], [], []\n",
    "    split_ratio = {'training': 0.71, 'validation': 0.09, 'testing': 0.2}\n",
    "    \n",
    "    for i, data_dir in enumerate(args['data_dirs']):\n",
    "        patient_lists = os.listdir( data_dir )\n",
    "        patient_lists.sort()\n",
    "        total_patients = len(patient_lists)\n",
    "\n",
    "        random.seed(5)\n",
    "        random.shuffle(patient_lists)\n",
    "    \n",
    "        train_split = int(split_ratio['training'] * total_patients)\n",
    "        val_split = int(split_ratio['validation'] * total_patients)\n",
    "    \n",
    "        train_patient_lists = patient_lists[:train_split]\n",
    "        val_patient_lists = patient_lists[train_split : train_split + val_split]\n",
    "        test_patient_lists = patient_lists[train_split + val_split :]\n",
    "    \n",
    "        train_patient_lists.sort()\n",
    "        val_patient_lists.sort()\n",
    "        test_patient_lists.sort()\n",
    "\n",
    "        # print(f'Train IDs of {data_dir}', train_patient_lists)\n",
    "        # print(f'Val IDs of {data_dir}', val_patient_lists)\n",
    "        print(f'Test IDs of {data_dir}', test_patient_lists)\n",
    "        \n",
    "        print(f'Number of training samples in {data_dir.split(\"/\")[3]} DataSet: {len(train_patient_lists)}')\n",
    "        print(f'Number of validation samples in {data_dir.split(\"/\")[3]} DataSet: {len(val_patient_lists)}')\n",
    "        print(f'Number of testing samples in {data_dir.split(\"/\")[3]} DataSet: {len(test_patient_lists)} ')\n",
    "\n",
    "        train_datasets.append(train_patient_lists)\n",
    "        val_datasets.append(val_patient_lists)\n",
    "        test_datasets.append(test_patient_lists)\n",
    "            \n",
    "    combined_trainDataset = combine_datasets(train_datasets, batch_size=args['train_batch_size'])\n",
    "    combined_valDataset = combine_datasets(val_datasets, batch_size=args['val_batch_size'])\n",
    "    combined_testDataset = combine_datasets(test_datasets, batch_size=args['test_batch_size'])\n",
    "    \n",
    "    print(f'Number of combined training samples', len(combined_trainDataset))\n",
    "    print(f'Number of combined validation samples', len(combined_valDataset))\n",
    "    print(f'Number of combined testing samples', len(combined_testDataset))\n",
    "    \n",
    "    trainDataset = CustomDataset3D( args['data_dirs'], combined_trainDataset, mode='training')\n",
    "    valDataset = CustomDataset3D( args['data_dirs'], combined_valDataset, mode='validation')\n",
    "    testDataset = CustomDataset3D( args['data_dirs'], combined_testDataset, mode='testing')\n",
    "    \n",
    "    trainLoader = DataLoader(\n",
    "        trainDataset, batch_size=args['train_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=False)\n",
    "    \n",
    "    valLoader = DataLoader(\n",
    "        valDataset, batch_size=args['val_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=False)\n",
    "    \n",
    "    testLoader = DataLoader(\n",
    "        testDataset, batch_size=args['test_batch_size'], num_workers=args['workers'], prefetch_factor=2,\n",
    "        pin_memory=True, shuffle=False)\n",
    "\n",
    "    return trainLoader, valLoader, testLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynUNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:44:43.125258Z",
     "iopub.status.busy": "2025-03-08T21:44:43.124615Z",
     "iopub.status.idle": "2025-03-08T21:44:43.146057Z",
     "shell.execute_reply": "2025-03-08T21:44:43.145256Z",
     "shell.execute_reply.started": "2025-03-08T21:44:43.125221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnetBasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN module module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        stride: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            dropout=dropout,\n",
    "            conv_only=True,\n",
    "        )\n",
    "\n",
    "        self.conv2 = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            conv_only=True\n",
    "        )\n",
    "        self.lrelu = get_act_layer(name=act_name)\n",
    "        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        out = self.norm1(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.lrelu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UnetUpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling module that can be used for DynUNet, based on:\n",
    "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
    "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        stride: convolution stride.\n",
    "        upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
    "        norm_name: feature normalization type and arguments.\n",
    "        act_name: activation layer type and arguments.\n",
    "        dropout: dropout probability.\n",
    "        trans_bias: transposed convolution bias.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[Sequence[int], int],\n",
    "        upsample_kernel_size: Union[Sequence[int], int],\n",
    "        norm_name: Union[Tuple, str] = (\"INSTANCE\", {\"affine\": True}),\n",
    "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "        trans_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        upsample_stride = upsample_kernel_size\n",
    "        \n",
    "        # ( a purple arrow in the paper )\n",
    "        self.transp_conv = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=upsample_kernel_size,\n",
    "            stride=upsample_stride,\n",
    "            dropout=dropout,\n",
    "            bias=trans_bias,\n",
    "            conv_only=True,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "        \n",
    "        # A light blue conv blocks in the decoder of nnUNet\n",
    "        self.conv_block = UnetBasicBlock(\n",
    "            spatial_dims,\n",
    "            out_channels + out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dropout=dropout,\n",
    "            norm_name=norm_name,\n",
    "            act_name=act_name,\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, skip):\n",
    "        # number of channels for skip should equals to out_channels\n",
    "        out = self.transp_conv(inp)\n",
    "        out = torch.cat((out, skip), dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UnetOutBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, in_channels: int, out_channels: int, dropout: Optional[Union[Tuple, str, float]] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = get_conv_layer(\n",
    "            spatial_dims, in_channels, out_channels, kernel_size=1, stride=1, dropout=dropout, bias=True, conv_only=True\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.conv(inp)\n",
    "    \n",
    "\n",
    "def get_conv_layer(\n",
    "    spatial_dims: int,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: Union[Sequence[int], int] = 3,\n",
    "    stride: Union[Sequence[int], int] = 1,\n",
    "    act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
    "    norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "    dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "    bias: bool = False,\n",
    "    conv_only: bool = True,\n",
    "    is_transposed: bool = False,\n",
    "):\n",
    "    padding = get_padding(kernel_size, stride)\n",
    "    output_padding = None\n",
    "    if is_transposed:\n",
    "        output_padding = get_output_padding(kernel_size, stride, padding)\n",
    "    \n",
    "    return Convolution(\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        strides=stride,\n",
    "        kernel_size=kernel_size,\n",
    "        act=act,\n",
    "        norm=norm,\n",
    "        dropout=dropout,\n",
    "        bias=bias,\n",
    "        conv_only=conv_only,\n",
    "        is_transposed=is_transposed,\n",
    "        padding=padding,\n",
    "        output_padding=output_padding,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = (kernel_size_np - stride_np + 1) / 2\n",
    "    if np.min(padding_np) < 0:\n",
    "        raise AssertionError(\"padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    padding = tuple(int(p) for p in padding_np)\n",
    "\n",
    "    return padding if len(padding) > 1 else padding[0]\n",
    "\n",
    "\n",
    "def get_output_padding(\n",
    "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]\n",
    ") -> Union[Tuple[int, ...], int]:\n",
    "    kernel_size_np = np.atleast_1d(kernel_size)\n",
    "    stride_np = np.atleast_1d(stride)\n",
    "    padding_np = np.atleast_1d(padding)\n",
    "\n",
    "    out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
    "    if np.min(out_padding_np) < 0:\n",
    "        raise AssertionError(\"out_padding value should not be negative, please change the kernel size and/or stride.\")\n",
    "    out_padding = tuple(int(p) for p in out_padding_np)\n",
    "\n",
    "    return out_padding if len(out_padding) > 1 else out_padding[0]\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:44:43.14735Z",
     "iopub.status.busy": "2025-03-08T21:44:43.146948Z",
     "iopub.status.idle": "2025-03-08T21:44:43.162913Z",
     "shell.execute_reply": "2025-03-08T21:44:43.162191Z",
     "shell.execute_reply.started": "2025-03-08T21:44:43.147312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DynUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        deep_supervision: bool,\n",
    "        KD: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.KD_enabled = KD\n",
    "        \n",
    "        self.input_conv = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=self.in_channels,\n",
    "                                     out_channels=64,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1\n",
    "                                     )\n",
    "        self.down1 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=64,\n",
    "                                     out_channels=96,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2 # Reduces spatial dims by 2\n",
    "                                     )\n",
    "        self.down2 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=96,\n",
    "                                     out_channels=128,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down3 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=128,\n",
    "                                     out_channels=192,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down4 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=192,\n",
    "                                     out_channels=256,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.down5 = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=256,\n",
    "                                     out_channels=384,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.bottleneck = UnetBasicBlock( spatial_dims=self.spatial_dims,\n",
    "                                     in_channels=384,\n",
    "                                     out_channels=512,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=2\n",
    "                                     )\n",
    "        self.up1 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=512,\n",
    "                                out_channels=384,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up2 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=384,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up3 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=256,\n",
    "                                out_channels=192,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.up4 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=192,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        \n",
    "        self.up5 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=128,\n",
    "                                out_channels=96,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )        \n",
    "        self.up6 = UnetUpBlock( spatial_dims=self.spatial_dims,\n",
    "                                in_channels=96,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=3,\n",
    "                                upsample_kernel_size=2\n",
    "                                )\n",
    "        self.out1 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=64,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out2 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=96,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        self.out3 = UnetOutBlock( spatial_dims=self.spatial_dims,\n",
    "                                  in_channels=128,\n",
    "                                  out_channels=self.out_channels,\n",
    "                                  )\n",
    "        \n",
    "    def forward( self, input ):\n",
    "        \n",
    "        # Input\n",
    "        x0 = self.input_conv( input ) # x0.shape = (B x 64 x 128 x 128 x 128)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.down1( x0 ) # x1.shape = (B x 96 x 64 x 64 x 64) \n",
    "        x2 = self.down2( x1 ) # x2.shape = (B x 128 x 32 x 32 x 32)\n",
    "        x3 = self.down3( x2 ) # x3.shape = (B x 192 x 16 x 16 x 16)\n",
    "        x4 = self.down4( x3 ) # x4.shape = (B x 256 x 8 x 8 x 8)   \n",
    "        x5 = self.down5( x4 ) # x5.shape = (B x 384 x 4 x 4 x 4)   \n",
    "        \n",
    "        # Bottleneck\n",
    "        x6 = self.bottleneck( x5 ) # x6.shape = (B x 512 x 2 x 2 x 2)\n",
    "        \n",
    "        # Decoder\n",
    "        x7  = self.up1( x6, x5 )  # x7.shape  = (B x 384 x 4 x 4 x 4)\n",
    "        x8  = self.up2( x7, x4 )  # x8.shape  = (B x 256 x 8 x 8 x 8)\n",
    "        x9  = self.up3( x8, x3 )  # x9.shape  = (B x 192 x 16 x 16 x 16)\n",
    "        x10 = self.up4( x9, x2 )  # x10.shape = (B x 128 x 32 x 32 x 32)\n",
    "        x11 = self.up5( x10, x1 ) # x11.shape = (B x 96 x 64 x 64 x 64)\n",
    "        x12 = self.up6( x11, x0 ) # x12.shape = (B x 64 x 128 x 128 x 128)\n",
    "        \n",
    "        # Output\n",
    "        output1 = self.out1( x12 )\n",
    "        \n",
    "        if (self.training and self.deep_supervision) or self.KD_enabled:\n",
    "            \n",
    "            # output['pred'].shape = B x 3 x 4 x 128 x 128 x 128\n",
    "            output2 = interpolate( self.out2( x11 ), output1.shape[2:])\n",
    "            output3 = interpolate( self.out3( x10 ), output1.shape[2:])\n",
    "            output_all = [ output1, output2, output3 ]\n",
    "            return { 'pred' : torch.stack(output_all, dim=1),\n",
    "                     'bottleneck_feature_map' : x6 }\n",
    "        \n",
    "        return { 'pred' : output1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "\n",
    "class ComputeMetrics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComputeMetrics, self).__init__()\n",
    "        self.dice_metric = DiceMetric(reduction=\"mean_batch\")\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(percentile=95.0, reduction=\"mean_batch\")\n",
    "\n",
    "    def compute(self, p, y, lbl):\n",
    "        self.dice_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "        \n",
    "        print(f\"{lbl} - Prediction unique values: {torch.unique(p)}\")\n",
    "        print(f\"{lbl} - Ground truth unique values: {torch.unique(y)}\")\n",
    "\n",
    "        if torch.sum(y.float()) == 0 and torch.sum(p.float()) == 0:  # True Negative Case: No foreground pixels in GT\n",
    "            print(f\"{lbl} - No positive samples in ground truth.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {1.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {0.0}\")\n",
    "            return torch.tensor(1.0), torch.tensor(0.0)\n",
    "        \n",
    "        if torch.sum(p.float()) == 0 and torch.sum(y.float()) > 0:  # False Negative Case: GT has 1s, Prediction is all 0s\n",
    "            print(f\"{lbl} - False Negative Case: GT has positive samples, but prediction is empty.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {0.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {373.1287}\")\n",
    "            return torch.tensor(0.0), torch.tensor(373.1287)\n",
    "        \n",
    "        if torch.sum(p.float()) > 0 and torch.sum(y.float()) == 0:  # False Positive Case: Prediction has 1s, GT is all 0s\n",
    "            print(f\"{lbl} - False Positive Case: Prediction has positives, but ground truth is empty.\")\n",
    "            print(f\"Dice scores for {lbl} for this batch: {0.0}\")\n",
    "            print(f\"Hausdorff distances for {lbl} for this batch: {373.1287}\")\n",
    "            return torch.tensor(0.0), torch.tensor(373.1287)\n",
    "\n",
    "        # Compute metrics normally\n",
    "        dice_score = self.dice_metric(p.float(), y.float())\n",
    "        hausdorff_dist = self.hausdorff_metric(p.float(), y.float())\n",
    "\n",
    "        print(f\"Dice scores for {lbl} for this batch:\\n {dice_score.item()}\")\n",
    "        print(f\"Hausdorff distances for {lbl} for this batch:\\n{hausdorff_dist.item()}\")\n",
    "    \n",
    "        return dice_score, hausdorff_dist\n",
    "\n",
    "    def forward(self, p, y):\n",
    "        p = (torch.sigmoid(p) > 0.5)\n",
    "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
    "        p_wt, p_tc, p_et = p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1), p[:, 3].unsqueeze(1)\n",
    "        \n",
    "        dice_wt, hd_wt = self.compute(p_wt, y_wt, 'wt')\n",
    "        dice_tc, hd_tc = self.compute(p_tc, y_tc, 'tc')\n",
    "        dice_et, hd_et = self.compute(p_et, y_et, 'et')\n",
    "        \n",
    "        return [dice_wt, hd_wt], [dice_tc, hd_tc], [dice_et, hd_et]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_net(model, loader): # student_model, testLoader\n",
    "    torch.manual_seed(0)\n",
    "    model.eval()\n",
    "    n_test_batches = len(loader)\n",
    "\n",
    "    compute_metrics = ComputeMetrics()\n",
    "    total_metrics = {\"WT\": {'dice_score': 0, 'hausdorff_distance': 0},\n",
    "                     \"TC\": {'dice_score': 0, 'hausdorff_distance': 0},\n",
    "                     \"ET\": {'dice_score': 0, 'hausdorff_distance': 0}}\n",
    "\n",
    "    with tqdm(total=n_test_batches, desc='Testing', unit='batch', leave=False) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for step, y in enumerate(loader):\n",
    "                y['imgs'], y['masks']= y['imgs'].to('cuda'), y['masks'].to('cuda')\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    print(\"--------Now patient:\", y['patient_id'])\n",
    "                    output = model(y['imgs'])\n",
    "                    wt_metrics, tc_metrics, et_metrics = compute_metrics(output['pred'], y['masks'])\n",
    "                    \n",
    "                    total_metrics['WT']['dice_score'] += wt_metrics[0].item()\n",
    "                    total_metrics['WT']['hausdorff_distance'] += wt_metrics[1].item()\n",
    "\n",
    "                    total_metrics['TC']['dice_score'] += tc_metrics[0].item()\n",
    "                    total_metrics['TC']['hausdorff_distance'] += tc_metrics[1].item()\n",
    "\n",
    "                    total_metrics['ET']['dice_score'] += et_metrics[0].item()\n",
    "                    total_metrics['ET']['hausdorff_distance'] += et_metrics[1].item()\n",
    "                                    \n",
    "                pbar.update(1)\n",
    "\n",
    "        total_metrics['WT']['dice_score'] /= n_test_batches\n",
    "        total_metrics['WT']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "        total_metrics['TC']['dice_score'] /= n_test_batches\n",
    "        total_metrics['TC']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "        total_metrics['ET']['dice_score'] /= n_test_batches\n",
    "        total_metrics['ET']['hausdorff_distance'] /= n_test_batches\n",
    "\n",
    "\n",
    "        print(\"************************************************************************\")\n",
    "        print(f\"Average Dice Score for WT: {total_metrics['WT']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for WT: {total_metrics['WT']['hausdorff_distance']:.4f}\")\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "                                     \n",
    "        print(f\"Average Dice Score for TC: {total_metrics['TC']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for TC: {total_metrics['TC']['hausdorff_distance']:.4f}\")\n",
    "                              \n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "                                     \n",
    "        print(f\"Average Dice Score for ET: {total_metrics['ET']['dice_score']:.4f}\")\n",
    "        print(f\"Average Hausdorff Distance for ET: {total_metrics['ET']['hausdorff_distance']:.4f}\")\n",
    "        print(\"************************************************************************\")\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on GLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    # 'data_dirs': [\"/kaggle/input/bratsglioma/Training/\", \"/kaggle/input/bratsafrica24/\", \"/kaggle/input/bratsped/Training/\", \"/kaggle/input/bratsmen/\", \"/kaggle/input/bratsmet24/\"],\n",
    "    'data_dirs': [\"/kaggle/input/bratsglioma/Training/\"],\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "student_path = Path(f'/kaggle/input/kd-5tumors-originalteachers-unbalanced-cbamkl/Student_model_after_epoch_60_trainLoss_0.8043_valLoss_0.3555.pth')\n",
    "student_model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "if (student_path).is_file():\n",
    "    print(f\"Found model: {student_path}\")\n",
    "    ckpt = torch.load(student_path, map_location='cuda', weights_only=True)\n",
    "    student_model.load_state_dict(ckpt['student_model'])\n",
    "    print(f\"Loaded model: {student_path}\")\n",
    "    test_net(student_model, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on SSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    # 'data_dirs': [\"/kaggle/input/bratsglioma/Training/\", \"/kaggle/input/bratsafrica24/\", \"/kaggle/input/bratsped/Training/\", \"/kaggle/input/bratsmen/\", \"/kaggle/input/bratsmet24/\"],\n",
    "    'data_dirs': [\"/kaggle/input/bratsafrica24/\"],\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "student_path = Path(f'/kaggle/input/kd-5tumors-originalteachers-unbalanced-cbamkl/Student_model_after_epoch_60_trainLoss_0.8043_valLoss_0.3555.pth')\n",
    "student_model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "if (student_path).is_file():\n",
    "    print(f\"Found model: {student_path}\")\n",
    "    ckpt = torch.load(student_path, map_location='cuda', weights_only=True)\n",
    "    student_model.load_state_dict(ckpt['student_model'])\n",
    "    print(f\"Loaded model: {student_path}\")\n",
    "    test_net(student_model, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on PED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    # 'data_dirs': [\"/kaggle/input/bratsglioma/Training/\", \"/kaggle/input/bratsafrica24/\", \"/kaggle/input/bratsped/Training/\", \"/kaggle/input/bratsmen/\", \"/kaggle/input/bratsmet24/\"],\n",
    "    'data_dirs': [\"/kaggle/input/bratsped/Training/\"],\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "student_path = Path(f'/kaggle/input/kd-5tumors-originalteachers-unbalanced-cbamkl/Student_model_after_epoch_60_trainLoss_0.8043_valLoss_0.3555.pth')\n",
    "student_model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "if (student_path).is_file():\n",
    "    print(f\"Found model: {student_path}\")\n",
    "    ckpt = torch.load(student_path, map_location='cuda', weights_only=True)\n",
    "    student_model.load_state_dict(ckpt['student_model'])\n",
    "    print(f\"Loaded model: {student_path}\")\n",
    "    test_net(student_model, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    # 'data_dirs': [\"/kaggle/input/bratsglioma/Training/\", \"/kaggle/input/bratsafrica24/\", \"/kaggle/input/bratsped/Training/\", \"/kaggle/input/bratsmen/\", \"/kaggle/input/bratsmet24/\"],\n",
    "    'data_dirs': [\"/kaggle/input/bratsmen/\"],\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "student_path = Path(f'/kaggle/input/kd-5tumors-originalteachers-unbalanced-cbamkl/Student_model_after_epoch_60_trainLoss_0.8043_valLoss_0.3555.pth')\n",
    "student_model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "if (student_path).is_file():\n",
    "    print(f\"Found model: {student_path}\")\n",
    "    ckpt = torch.load(student_path, map_location='cuda', weights_only=True)\n",
    "    student_model.load_state_dict(ckpt['student_model'])\n",
    "    print(f\"Loaded model: {student_path}\")\n",
    "    test_net(student_model, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'workers': 2,\n",
    "    'epochs': 15,\n",
    "    'train_batch_size': 2,\n",
    "    'val_batch_size': 2,\n",
    "    'test_batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    # 'data_dirs': [\"/kaggle/input/bratsglioma/Training/\", \"/kaggle/input/bratsafrica24/\", \"/kaggle/input/bratsped/Training/\", \"/kaggle/input/bratsmen/\", \"/kaggle/input/bratsmet24/\"],\n",
    "    'data_dirs': [\"/kaggle/input/bratsmet24/\"],\n",
    "}\n",
    "\n",
    "_, _, testLoader = prepare_data_loaders(args)\n",
    "student_path = Path(f'/kaggle/input/kd-5tumors-originalteachers-unbalanced-cbamkl/Student_model_after_epoch_60_trainLoss_0.8043_valLoss_0.3555.pth')\n",
    "student_model = DynUNet( spatial_dims=3, in_channels=4, out_channels=4, deep_supervision=False).to('cuda')\n",
    "if (student_path).is_file():\n",
    "    print(f\"Found model: {student_path}\")\n",
    "    ckpt = torch.load(student_path, map_location='cuda', weights_only=True)\n",
    "    student_model.load_state_dict(ckpt['student_model'])\n",
    "    print(f\"Loaded model: {student_path}\")\n",
    "    test_net(student_model, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Press here"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5906271,
     "sourceId": 9666149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5929819,
     "sourceId": 9697878,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5984455,
     "sourceId": 9770532,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6060139,
     "sourceId": 9871878,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6235428,
     "sourceId": 10107956,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6246431,
     "sourceId": 10123910,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6247793,
     "sourceId": 10129981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6329313,
     "sourceId": 10235842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6343297,
     "sourceId": 10254632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6342439,
     "sourceId": 10262775,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6825682,
     "sourceId": 10974831,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6842116,
     "sourceId": 11006305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6874260,
     "sourceId": 11054610,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6927092,
     "sourceId": 11148300,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
